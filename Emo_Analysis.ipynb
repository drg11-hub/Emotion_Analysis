{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emo_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAsHGSxqlJDH",
        "colab_type": "text"
      },
      "source": [
        "Importing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb_lZVyzlNK8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "470e427c-0761-4a30-b7f8-112daedb9135"
      },
      "source": [
        "import nltk\n",
        "!pip install smart_open\n",
        "from smart_open import open\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import *\n",
        "from textblob.classifiers import NaiveBayesClassifier\n",
        "from sklearn.model_selection import KFold\n",
        "from nltk.classify.naivebayes import NaiveBayesClassifier\n",
        "from gensim import corpora, models, similarities\n",
        "from nltk.corpus import wordnet as wn\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart_open) (1.14.18)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart_open) (2.23.0)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart_open) (2.49.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.18 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open) (1.17.18)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart_open) (0.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart_open) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart_open) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart_open) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart_open) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.18->boto3->smart_open) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.18->boto3->smart_open) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.18->boto3->smart_open) (1.12.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jidbeDLZlnex",
        "colab_type": "text"
      },
      "source": [
        "Reading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W15whENBlLEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Read dataset ISEAR.csv - Dataset\n",
        "Dataset = pd.read_csv('ISEAR_raw.csv',header=None)\n",
        "\n",
        "#Emotions to be detected - det_emo\n",
        "det_emo = ['anger', 'disgust', 'fear', 'guilt', 'joy', 'sadness', 'shame']\n",
        "\n",
        "#Negation words - neg_words\n",
        "neg_words = ['not', 'neither', 'nor', 'never', 'but', 'however', 'although', 'nonetheless', 'despite', 'except', 'even though', 'yet']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcYJE_n_2DYC",
        "colab_type": "text"
      },
      "source": [
        "Displaying Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igUL6fVr2GAN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "3c50b88a-8575-4b42-fa64-bf12b1e86346"
      },
      "source": [
        "Dataset.head(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>joy</td>\n",
              "      <td>On days when I feel close to my partner and ot...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fear</td>\n",
              "      <td>Every time I imagine that someone I love or I ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>When I had been obviously unjustly treated and...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sadness</td>\n",
              "      <td>When I think about the short time that we live...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>disgust</td>\n",
              "      <td>At a gathering I found myself involuntarily si...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>shame</td>\n",
              "      <td>When I realized that I was directing the feeli...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>guilt</td>\n",
              "      <td>I feel guilty when when I realize that I consi...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>joy</td>\n",
              "      <td>After my girlfriend had taken her exam we went...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>fear</td>\n",
              "      <td>When, for the first time I realized the meanin...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>anger</td>\n",
              "      <td>When a car is overtaking another and I am forc...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0                                                  1    2\n",
              "0      joy  On days when I feel close to my partner and ot...  NaN\n",
              "1     fear  Every time I imagine that someone I love or I ...  NaN\n",
              "2    anger  When I had been obviously unjustly treated and...  NaN\n",
              "3  sadness  When I think about the short time that we live...  NaN\n",
              "4  disgust  At a gathering I found myself involuntarily si...  NaN\n",
              "5    shame  When I realized that I was directing the feeli...  NaN\n",
              "6    guilt  I feel guilty when when I realize that I consi...  NaN\n",
              "7      joy  After my girlfriend had taken her exam we went...  NaN\n",
              "8     fear  When, for the first time I realized the meanin...  NaN\n",
              "9    anger  When a car is overtaking another and I am forc...  NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyTfTFVQlzKG",
        "colab_type": "text"
      },
      "source": [
        "Cleaning the Dataset, Stemming, POS-TAGGER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U_2qO_ll8WH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "9051f25f-c50f-4df8-db94-8addd9ef25f6"
      },
      "source": [
        "#Removes unnecessary characters from sentences\n",
        "#Cleaning of Data\n",
        "def removal(sentences):\n",
        "  sentence_list = []\n",
        "  count = 0\n",
        "  sent = nltk.word_tokenize(sentences)\n",
        "  chars = [\"รก\", \"\\xc3\", \"\\xa1\", \"\\n\", \",\", \".\", \"[\", \"]\", \"\"]\n",
        "  clean_list = []\n",
        "  for i in sent:\n",
        "    if i not in chars:\n",
        "      clean_list.append(i)\n",
        "  return clean_list\n",
        "\n",
        "\n",
        "#POS-TAGGER and returns NAVA words\n",
        "def pos_tag(sentences):\n",
        "  tags = [] #have the pos tag included\n",
        "  nava_sen = []\n",
        "  pt = nltk.pos_tag(sentences)\n",
        "  nava = []\n",
        "  nava_words = []\n",
        "  for t in pt:\n",
        "    if t[1].startswith('NN') or t[1].startswith('JJ') or t[1].startswith('VB') or t[1].startswith('RB'):\n",
        "      nava.append(t)\n",
        "      nava_words.append(t[0])\n",
        "  return nava, nava_words\n",
        "\n",
        "\n",
        "#Performs stemming\n",
        "def stemming(sentences):\n",
        "  sent_list = []\n",
        "  sent_string = []\n",
        "  sent_token = []\n",
        "  stemmer = PorterStemmer()\n",
        "  #temp = 0\n",
        "  #temp += 1\n",
        "  temp = 1\n",
        "  st = \"\"\n",
        "  for word in sentences:\n",
        "    word_lower = word.lower()\n",
        "    if len(word_lower) >= 3:\n",
        "      st += stemmer.stem(word_lower) + \" \"\n",
        "  sent_string.append(st)\n",
        "  word_set = nltk.word_tokenize(st)\n",
        "  sent_token.append(word_set)\n",
        "  word_text = nltk.Text(word_set)\n",
        "  sent_list.append(word_text)\n",
        "  return word_text, st, word_set\n",
        "#   return sentence_list, sen_string, sen_token\n",
        "\n",
        "\n",
        "'''def removal(input_text):\n",
        "    a=[]\n",
        "    a1 = re.sub(r'@\\w+', '', input_text)\n",
        "    a2 = re.sub(r'http.?://[^\\s]+[\\s]?', '', a1)\n",
        "    punct = string.punctuation\n",
        "    trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
        "    a3 = a2.translate(trantab)\n",
        "    a4 = re.sub('\\d+', '', a3)\n",
        "    a4 = a4.lower()\n",
        "    return a4'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"def removal(input_text):\\n    a=[]\\n    a1 = re.sub(r'@\\\\w+', '', input_text)\\n    a2 = re.sub(r'http.?://[^\\\\s]+[\\\\s]?', '', a1)\\n    punct = string.punctuation\\n    trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\\n    a3 = a2.translate(trantab)\\n    a4 = re.sub('\\\\d+', '', a3)\\n    a4 = a4.lower()\\n    return a4\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCD8hmoJmVqs",
        "colab_type": "text"
      },
      "source": [
        "Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NptGGgEymbsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Write to file\n",
        "def write_to_file(filename, text):\n",
        "  o = open(filename,'w')\n",
        "  o.write(str(text))\n",
        "  o.close()\n",
        "\n",
        "\n",
        "#Reads the emotion representative words file\n",
        "def readfile(filename):\n",
        "  f = open(filename,'r')\n",
        "  representative_words = []\n",
        "  for line in f.readlines():\n",
        "    characters = [\"\\n\", \" \", \"\\r\", \"\\t\"]\n",
        "    new = ''.join([i for i in line if not [e for e in characters if e in i]])\n",
        "    representative_words.append(new)\n",
        "  return representative_words\n",
        "\n",
        "\n",
        "#Makes a list of all words semantically related to an emotion and Stemming\n",
        "def affect_wordlist(words):\n",
        "  affect_words = []\n",
        "  stemmer = PorterStemmer()\n",
        "  for w in words:\n",
        "    w_l = w.lower()\n",
        "    word_stem = stemmer.stem(w_l)\n",
        "    if word_stem not in affect_words:\n",
        "      affect_words.append(word_stem)\n",
        "  return affect_words\n",
        "\n",
        "\n",
        "#Creating an emotion wordset\n",
        "def emotion_word_set(emotions):\n",
        "  word_set = {}\n",
        "  for e in emotions:\n",
        "    representative_words = readfile(e)\n",
        "    wordlist = affect_wordlist(representative_words)\n",
        "    word_set[e] = wordlist\n",
        "  return word_set\n",
        "\n",
        "\n",
        "#Emotion Detector - Getting synonyms from wordnet synsets\n",
        "def get_synonyms():\n",
        "  syn = {}\n",
        "  for e in emotion_labels:\n",
        "    jw = wn.synsets(e)\n",
        "    for s in jw:\n",
        "      v = s.name()\n",
        "      try:\n",
        "        syn[e].append(wn.synset(v).lemma_names())\n",
        "      except KeyError:\n",
        "        syn[e] = wn.synset(v).lemma_names()\n",
        "\n",
        "                \n",
        "#Emotion Detector - Creating training/testing set for Naive Bayes classifier TextBlob -- Not used\n",
        "def create_dataset_textblob(sentences, emotions):\n",
        "  train = []\n",
        "  sen = []\n",
        "  emo = []\n",
        "  for s in sentences:\n",
        "    sen.append(s)\n",
        "  for e in emotions:\n",
        "    emo.append(e)\n",
        "  for i in range(len(sen)):\n",
        "    s = sen[i]\n",
        "    e = emo[i]\n",
        "    train.append((str(s), e))\n",
        "  return train\n",
        "\n",
        "\n",
        "#Emotion Detector - Creating training/testing set for Naive Bayes classifier TextBlob -- Not used\n",
        "def create_dataset_textblob(sentences, emotions):\n",
        "  train = []\n",
        "  sen = []\n",
        "  emo = []\n",
        "  for s in sentences:\n",
        "    sen.append(s)\n",
        "  for e in emotions:\n",
        "    emo.append(e)\n",
        "  for i in range(len(sen)):\n",
        "    s = sen[i]\n",
        "    e = emo[i]\n",
        "    train.append((str(s), e))\n",
        "  return train\n",
        "\n",
        "\n",
        "#Create dataset for nltk Naive Bayes\n",
        "def create_data(sentence, emotion):\n",
        "  data = []\n",
        "  for i in range(len(sentence)):\n",
        "    sen = []\n",
        "    for s in sentence[i]:\n",
        "      sen.append(str(s))\n",
        "    emo = emotion[i]\n",
        "    data.append((sen, emo))\n",
        "  return data\n",
        "\n",
        "\n",
        "#Get all words in dataset\n",
        "def get_words_in_dataset(dataset):\n",
        "  all_words = []\n",
        "  for (words, sentiment) in dataset:\n",
        "    all_words.extend(words)\n",
        "  return all_words\n",
        "\n",
        "\n",
        "#Getting frequency dist of words\n",
        "def get_word_features(wordlist):\n",
        "  wordlist = nltk.FreqDist(wordlist)\n",
        "  word_features = wordlist.keys()\n",
        "  return word_features\n",
        "\n",
        "\n",
        "#Testing for Naive Bayes Classifier\n",
        "def testing(cl, test):\n",
        "  for s, e in test:\n",
        "    r = cl.classify(s)\n",
        "    print(s, e, r)\n",
        "    if r == e:\n",
        "      print(\"*\")\n",
        "            \n",
        "            \n",
        "#Extacting features\n",
        "def extract_features(document):\n",
        "  document_words = set(document)\n",
        "  features = {}\n",
        "  for word in word_features:\n",
        "    features['contains(%s)' % word] = (word in document_words)\n",
        "  return features\n",
        "\n",
        "\n",
        "#Create test data\n",
        "def create_test(sentence, emotion):\n",
        "  data = []\n",
        "  sen = []\n",
        "  emo = []\n",
        "  for s in sentence:\n",
        "    sen.append(str(s))\n",
        "  for e in emotion:\n",
        "    emo.append(e)\n",
        "  for i in range(len(sen)):\n",
        "    temp = []\n",
        "    temp.append(sen[i])\n",
        "    temp.append(emo[i])\n",
        "    data.append(temp)\n",
        "  return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x4HbS-_mkHd",
        "colab_type": "text"
      },
      "source": [
        "Creating the Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yolKKJXmphX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the dataframe\n",
        "def create_frame(Data):\n",
        "  labels = []\n",
        "  sen = []\n",
        "  sen_str = []\n",
        "  sen_tok = []\n",
        "  labelset = []\n",
        "  for i in range(len(Data)):\n",
        "    if i >= 0:\n",
        "      emotion = Data[0][i]\n",
        "      data_toclean = Data[1][i]\n",
        "      labels.append(emotion)\n",
        "      labelset.append([emotion])\n",
        "      sent = removal(data_toclean)\n",
        "      nava, sent_pt = pos_tag(sent)\n",
        "      sentences, sen_string, sen_token = stemming(sent_pt)\n",
        "      sen.append(sentences)\n",
        "      sen_str.append(sen_string)\n",
        "      sen_tok.append(sen_token)\n",
        "  df = pd.DataFrame({0 : labels,\n",
        "                        1 : sen,\n",
        "                        2 : sen_str,\n",
        "                        3 : sen_tok,\n",
        "                        4 : labelset})\n",
        "  return df, sen_tok, labels, sen_str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elf5PrDTmygu",
        "colab_type": "text"
      },
      "source": [
        "Displaying result of calling create_frame function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjCTd7s6mwb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calling the create_frame function\n",
        "c, st, labels, review_sent = create_frame(Dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw1_g1t5m9CP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "72790c5c-29c1-4895-8589-5817f8673a08"
      },
      "source": [
        "c"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>joy</td>\n",
              "      <td>(day, feel, close, partner, other, friend, fee...</td>\n",
              "      <td>day feel close partner other friend feel peac ...</td>\n",
              "      <td>[day, feel, close, partner, other, friend, fee...</td>\n",
              "      <td>[joy]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fear</td>\n",
              "      <td>(time, imagin, someon, love, contact, seriou, ...</td>\n",
              "      <td>time imagin someon love contact seriou ill eve...</td>\n",
              "      <td>[time, imagin, someon, love, contact, seriou, ...</td>\n",
              "      <td>[fear]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>anger</td>\n",
              "      <td>(had, been, obvious, unjustli, treat, had, pos...</td>\n",
              "      <td>had been obvious unjustli treat had possibl el...</td>\n",
              "      <td>[had, been, obvious, unjustli, treat, had, pos...</td>\n",
              "      <td>[anger]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sadness</td>\n",
              "      <td>(think, short, time, live, relat, period, life...</td>\n",
              "      <td>think short time live relat period life think ...</td>\n",
              "      <td>[think, short, time, live, relat, period, life...</td>\n",
              "      <td>[sadness]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>disgust</td>\n",
              "      <td>(gather, found, involuntarili, sit, next, peop...</td>\n",
              "      <td>gather found involuntarili sit next peopl expr...</td>\n",
              "      <td>[gather, found, involuntarili, sit, next, peop...</td>\n",
              "      <td>[disgust]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7511</th>\n",
              "      <td>shame</td>\n",
              "      <td>(year, back, someon, invit, tutor, grand-daugh...</td>\n",
              "      <td>year back someon invit tutor grand-daught gran...</td>\n",
              "      <td>[year, back, someon, invit, tutor, grand-daugh...</td>\n",
              "      <td>[shame]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7512</th>\n",
              "      <td>shame</td>\n",
              "      <td>(had, taken, respons, someth, had, prepar, how...</td>\n",
              "      <td>had taken respons someth had prepar howev fail...</td>\n",
              "      <td>[had, taken, respons, someth, had, prepar, how...</td>\n",
              "      <td>[shame]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7513</th>\n",
              "      <td>fear</td>\n",
              "      <td>(wa, home, heard, loud, sound, spit, door, tho...</td>\n",
              "      <td>wa home heard loud sound spit door thought fam...</td>\n",
              "      <td>[wa, home, heard, loud, sound, spit, door, tho...</td>\n",
              "      <td>[fear]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7514</th>\n",
              "      <td>guilt</td>\n",
              "      <td>(did, not, homework, teacher, had, ask, wa, sc...</td>\n",
              "      <td>did not homework teacher had ask wa scold immedi</td>\n",
              "      <td>[did, not, homework, teacher, had, ask, wa, sc...</td>\n",
              "      <td>[guilt]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7515</th>\n",
              "      <td>fear</td>\n",
              "      <td>(had, shout, younger, brother, wa, alway, afra...</td>\n",
              "      <td>had shout younger brother wa alway afraid call...</td>\n",
              "      <td>[had, shout, younger, brother, wa, alway, afra...</td>\n",
              "      <td>[fear]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7516 rows ร 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0  ...          4\n",
              "0         joy  ...      [joy]\n",
              "1        fear  ...     [fear]\n",
              "2       anger  ...    [anger]\n",
              "3     sadness  ...  [sadness]\n",
              "4     disgust  ...  [disgust]\n",
              "...       ...  ...        ...\n",
              "7511    shame  ...    [shame]\n",
              "7512    shame  ...    [shame]\n",
              "7513     fear  ...     [fear]\n",
              "7514    guilt  ...    [guilt]\n",
              "7515     fear  ...     [fear]\n",
              "\n",
              "[7516 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMdekmGymrGi",
        "colab_type": "text"
      },
      "source": [
        "Defining Function for Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-Pty95mnMj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Classifier\n",
        "def classify_dataset(data):\n",
        "  return classifier.classify(extract_features(nltk.word_tokenize(data)))\n",
        "\n",
        "\n",
        "#Get accuracy\n",
        "def get_accuracy(test_data, classifier):\n",
        "  total = accuracy = float(len(test_data))\n",
        "  for data in test_data:\n",
        "    if classify_dataset(data[0]) != data[1]:\n",
        "      accuracy -= 1\n",
        "  #print('Accuracy with Naive Bayes Classifier is: (%d/20) = %f%%.' % (accuracy, accuracy / total * 100))\n",
        "  result = accuracy / total * 100\n",
        "  return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrq1k32qnPvc",
        "colab_type": "text"
      },
      "source": [
        "Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsN7E1sMncQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create training and testing data\n",
        "sen = c[3]\n",
        "emo = c[0]\n",
        "l = len(c[3])\n",
        "limit = (9*l)//10\n",
        "sente = c[2]\n",
        "Data = create_data(sen[:limit], emo[:limit])\n",
        "test_data = create_test(sente[limit:], emo[limit:])\n",
        "\n",
        "\n",
        "# extract the word features out from the training data\n",
        "word_features = get_word_features(get_words_in_dataset(Data))\n",
        "\n",
        "\n",
        "# get the training set and train the Naive Bayes Classifier\n",
        "training_set = nltk.classify.util.apply_features(extract_features, Data)\n",
        "classifier = NaiveBayesClassifier.train(training_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMSmeIdanfw0",
        "colab_type": "text"
      },
      "source": [
        "Final result ( accuracy )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPTuSx0Jnvs3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12aab521-a03c-44b1-de40-39a582e0ae53"
      },
      "source": [
        "# Calling get accuracy function to know Accuracy\n",
        "res = get_accuracy(test_data, classifier)\n",
        "print(\"Accuracy using Naive Bayes Component  \", res, \"%\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy using Naive Bayes Component   63.16489361702128 %\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}